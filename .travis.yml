---
sudo: required
dist: bionic
env:
  global:
  # auto vagrant installation
  - VAGRANT_CURRENT_VERSION="$(curl -s https://checkpoint-api.hashicorp.com/v1/check/vagrant | jq -r -M '.current_version')"
  # auto vagrant installation
  - VAGRANT_VERSION="2.2.9"
  - KUBECTL_VERSION=1.18.3
  - KUBERNETES_VERSION=1.18.3
  - MINIKUBE_VERSION=1.8.3
  - CHANGE_MINIKUBE_NONE_USER=true #(bool) automatically change ownership of ~/.minikube to the value of $SUDO_USER https://minikube.sigs.k8s.io/docs/handbook/config/
  - MINIKUBE_WANTREPORTERRORPROMPT=false
  - MINIKUBE_WANTUPDATENOTIFICATION=false #(bool) sets whether the user wants an update notification for new minikube versions https://minikube.sigs.k8s.io/docs/handbook/config
  - MINIKUBE_HOME=$HOME #(string) sets the path for the .minikube directory that minikube uses for state/configuration. Please note: this is used only by minikube https://minikube.sigs.k8s.io/docs/handbook/config
  - KUBECONFIG=$HOME/.kube/config

notifications:
  slack:
    on_failure: always

#https://istio.io/docs/setup/platform-setup/microk8s/
fleet_script_microk8s_istio_tasks : &fleet_script_microk8s_istio_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - sudo snap install microk8s --classic #Install the latest version of MicroK8s
          - sudo microk8s.enable istio #Enable Istio
          - watch microk8s.kubectl get all --all-namespaces #check deployment progress

#https://istio.io/docs/setup/platform-setup/gardener/
#https://github.com/gardener/gardener/blob/master/docs/development/local_setup.md
fleet_script_minikube_gadener_tasks : &fleet_script_minikube_gadener_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - sudo snap install helm --classic
          - sudo apt-get -qqy install openvpn
          - egrep -c '(vmx|svm)' /proc/cpuinfo | echo "virtualization is  supported" | echo "virtualization is not supported"
          - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
          - curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
          - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
          - mkdir -p $HOME/.kube $HOME/.minikube
          - touch $KUBECONFIG
          - sudo snap install kubectl --classic && kubectl version –client
          - echo "=====================Local Gardener setup Using minikube====================================================================="
          - minikube config set embed-certs true
          - minikube start --profile=minikube --vm-driver=none --kubernetes-version=v$KUBERNETES_VERSION --embed-certs #  `--embed-certs` can be omitted if minikube has already been set to create self-contained kubeconfig files.
          - make dev-setup #point your KUBECONFIG environment variable to the local cluster you created in the previous step and run
          - kubectl apply -f example/10-secret-internal-domain-unmanaged.yaml
          - make start-apiserver
          - make start-controller-manager
          - make start-scheduler
          - make start-gardenlet
          - kubectl get shoots
          - echo "=========================================================================================="
          # - sudo minikube start --profile=minikube --vm-driver=none --kubernetes-version=v$KUBERNETES_VERSION #the none driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
          - minikube update-context --profile=minikube
          - "sudo chown -R travis: /home/travis/.minikube/"
          - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
          - echo "=========================================================================================="
          - kubectl version
          - kubectl version --client #the version of the client
          - kubectl cluster-info
          - minikube status
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
                 kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
                break
              fi
              sleep 2
            done
          - echo "============================status check=============================================================="
          - minikube status
          - kubectl cluster-info
          - kubectl get pods --all-namespaces;
          - kubectl get pods -n default;
          - echo "===========================Local Gardener setup==============================================================="
          - git clone git@github.com:gardener/gardener.git && cd gardener #Local Gardener setup
          - sudo make local-garden-up #Using the nodeless cluster setup
          - sudo make local-garden-down # tear down the local Garden cluster and remove the Docker containers
          - echo "=========================================================================================="

#https://istio.io/docs/setup/platform-setup/gardener/
#https://github.com/gardener/gardener/blob/master/docs/development/local_setup.md
fleet_script_kind_gardener_tasks : &fleet_script_kind_gardener_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64
          - chmod +x ./kind
          - sudo mv ./kind /usr/local/bin/kind
          - kind get clusters #see the list of kind clusters
          - kind create cluster --name istio-testing #Create a cluster,By default, the cluster will be given the name kind
          - kind get clusters
          - sudo snap install kubectl --classic
          - kubectl config get-contexts #list the local Kubernetes contexts
          - kubectl config use-context kind-istio-testing #run following command to set the current context for kubectl
          - kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml #deploy Dashboard
          - echo "===============================Waiting for Dashboard to be ready==========================================================="
          - |
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if kubectl get pods --namespace=kubernetes-dashboard | grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get pod -n kubernetes-dashboard #Verify that Dashboard is deployed and running
          - kubectl create clusterrolebinding default-admin --clusterrole cluster-admin --serviceaccount=default:default #Create a ClusterRoleBinding to provide admin access to the newly created cluster
          #To login to Dashboard, you need a Bearer Token. Use the following command to store the token in a variable
          - token=$(kubectl get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}"|base64 --decode)
          - echo $token #Display the token using the echo command and copy it to use for logging into Dashboard.
          - kubectl proxy & # Access Dashboard using the kubectl command-line tool by running the following command, Starting to serve on 127.0.0.1:8001
          - |
            for i in {1..60}; do # Timeout after 1 mins, 60x1=60 secs
              if nc -z -v 127.0.0.1 8001 2>&1 | grep succeeded ; then
                break
              fi
              sleep 1
            done
          # - kind delete cluster --name istio-testing #delete the existing cluster


#https://istio.io/docs/setup/platform-setup/kind/
#https://kind.sigs.k8s.io/docs/user/quick-start/
#https://istio.io/docs/setup/getting-started/
fleet_script_kind_istio_tasks : &fleet_script_kind_istio_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - docker version
          - curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64
          - chmod +x ./kind
          - sudo mv ./kind /usr/local/bin/kind
          - kind get clusters #see the list of kind clusters
          - kind create cluster --name istio-testing #Create a cluster,By default, the cluster will be given the name kind
          - kind get clusters
          - sudo snap install kubectl --classic
          - kubectl config get-contexts #list the local Kubernetes contexts
          - kubectl config use-context kind-istio-testing #run following command to set the current context for kubectl
          - kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml #deploy Dashboard
          - echo "===============================Waiting for Dashboard to be ready==========================================================="
          - kubectl get service --all-namespaces #list all services in all namespace
          - |
            for i in {1..60}; do # Timeout after 5 minutes, 60x2=120 secs, 2 mins
              if kubectl get pods --namespace=kubernetes-dashboard |grep Running && \
                 kubectl get pods --namespace=dashboard-metrics-scraper |grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get pod -n kubernetes-dashboard #Verify that Dashboard is deployed and running
          - kubectl create clusterrolebinding default-admin --clusterrole cluster-admin --serviceaccount=default:default #Create a ClusterRoleBinding to provide admin access to the newly created cluster
          #To login to Dashboard, you need a Bearer Token. Use the following command to store the token in a variable
          - token=$(kubectl get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}"|base64 --decode)
          - echo $token #Display the token using the echo command and copy it to use for logging into Dashboard.
          - sudo apt-get install net-tools -qqy #Install netcat
          - kubectl proxy & # Access Dashboard using the kubectl command-line tool by running the following command, Starting to serve on 127.0.0.1:8001
          - |
            for i in {1..60}; do # Timeout after 1 mins, 60x1=60 secs
              if nc -z -v 127.0.0.1 8001 2>&1 | grep succeeded ; then
                break
              fi
              sleep 1
            done
          - echo "===============================Install istio==========================================================="
          - 'curl -L https://istio.io/downloadIstio | sh -' #Download Istio
          -  cd istio-* #Move to the Istio package directory. For example, if the package is istio-1.6.0
          - export PATH=$PWD/bin:$PATH #Add the istioctl client to your path, The istioctl client binary in the bin/ directory.
          #precheck inspects a Kubernetes cluster for Istio install requirements
          - istioctl experimental precheck #https://istio.io/docs/reference/commands/istioctl/#istioctl-experimental-precheck
          #Begin the Istio pre-installation verification check
          # - istioctl verify-install #Error: could not load IstioOperator from cluster: the server could not find the requested resource.  Use --filename
          - istioctl version
          - istioctl manifest apply --set profile=demo #Install Istio, use the demo configuration profile
          - kubectl label namespace default istio-injection=enabled #Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later
          - kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml #Deploy the Bookinfo sample application:
          - kubectl get service --all-namespaces #list all services in all namespace
          - kubectl get services #The application will start. As each pod becomes ready, the Istio sidecar will deploy along with it.
          - kubectl get pods
          - |
            for i in {1..60}; do # Timeout after 5 minutes, 60x2=120 secs, 2 mins
              if kubectl get pods --namespace=istio-system |grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get service --all-namespaces #list all services in all namespace
          # - |
          #   kubectl exec -it $(kubectl get pod \
          #                -l app=ratings \
          #                -o jsonpath='{.items[0].metadata.name}') \
          #                -c ratings \
          #                -- curl productpage:9080/productpage | grep -o "<title>.*</title>" <title>Simple Bookstore App</title>
          #Open the application to outside traffic
          #The Bookinfo application is deployed but not accessible from the outside. To make it accessible, you need to create an Istio Ingress Gateway, which maps a path to a route at the edge of your mesh.
          # - kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml #Associate this application with the Istio gateway
          # - istioctl analyze #Ensure that there are no issues with the configuration
          #Determining the ingress IP and ports
          #If the EXTERNAL-IP value is set, your environment has an external load balancer that you can use for the ingress gateway.
          # - kubectl get svc istio-ingressgateway -n istio-system #determine if your Kubernetes cluster is running in an environment that supports external load balancers
          # #Follow these instructions if you have determined that your environment has an external load balancer.
          # # If the EXTERNAL-IP value is <none> (or perpetually <pending>), your environment does not provide an external load balancer for the ingress gateway,access the gateway using the service’s node port.
          # - export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          # - export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
          # - export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
          # #In certain environments, the load balancer may be exposed using a host name, instead of an IP address.
          # #the ingress gateway’s EXTERNAL-IP value will not be an IP address, but rather a host name
          # #failed to set the INGRESS_HOST environment variable, correct the INGRESS_HOST value
          # - export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          # #Follow these instructions if your environment does not have an external load balancer and choose a node port instead
          # - export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}') #Set the ingress ports
          # - export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}') #Set the ingress ports
          # - export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT #Set GATEWAY_URL
          # - echo $GATEWAY_URL #Ensure an IP address and port were successfully assigned to the environment variable
          # # - echo http://$GATEWAY_URL/productpage #Verify external access,retrieve the external address of the Bookinfo application
          # - istioctl dashboard kiali #optional dashboards installed by the demo installation,Access the Kiali dashboard. The default user name is admin and default password is admin
          # #The Istio uninstall deletes the RBAC permissions and all resources hierarchically under the istio-system namespace
          # #It is safe to ignore errors for non-existent resources because they may have been deleted hierarchically.
          # - 'istioctl manifest generate --set profile=demo | kubectl delete -f -'
          # - kubectl delete namespace istio-system #The istio-system namespace is not removed by default. If no longer needed, use the following command to remove it
          # - kubectl get virtualservices   #-- there should be no virtual services
          # - kubectl get destinationrules  #-- there should be no destination rules
          # - kubectl get gateway           #-- there should be no gateway
          # - kubectl get pods              #-- the Bookinfo pods should be deleted
          # #Bookinfo cleanup starts
          # - |
          #   SCRIPTDIR=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )
          #   # only ask if in interactive mode
          #   if [[ -t 0 && -z ${NAMESPACE} ]];then
          #     echo -n "namespace ? [default] "
          #     read -r NAMESPACE
          #   fi
          #   if [[ -z ${NAMESPACE} ]];then
          #     NAMESPACE=default
          #   fi
          #   echo "using NAMESPACE=${NAMESPACE}"
          #   protos=( destinationrules virtualservices gateways )
          #   for proto in "${protos[@]}"; do
          #     for resource in $(kubectl get -n ${NAMESPACE} "$proto" -o name); do
          #       kubectl delete -n ${NAMESPACE} "$resource";
          #     done
          #   done
          #   OUTPUT=$(mktemp)
          #   export OUTPUT
          #   echo "Application cleanup may take up to one minute"
          #   kubectl delete -n ${NAMESPACE} -f "$SCRIPTDIR/bookinfo.yaml" > "${OUTPUT}" 2>&1
          #   ret=$?
          #   function cleanup() {
          #     rm -f "${OUTPUT}"
          #   }
          #   trap cleanup EXIT
          #   if [[ ${ret} -eq 0 ]];then
          #     cat "${OUTPUT}"
          #   else
          #     # ignore NotFound errors
          #     OUT2=$(grep -v NotFound "${OUTPUT}")
          #     if [[ -n ${OUT2} ]];then
          #       cat "${OUTPUT}"
          #       exit ${ret}
          #     fi
          #   fi
          #   echo "Application cleanup successful"
          # - kubectl get virtualservices   #-- there should be no virtual services
          # - kubectl get destinationrules  #-- there should be no destination rules
          # - kubectl get gateway           #-- there should be no gateway
          # - kubectl get pods              #-- the Bookinfo pods should be deleted
          #Bookinfo cleanup ends
          # - echo "===============================Adding Heapster Metrics to the Kubernetes Dashboard==========================================================="
          # - sudo snap install helm --classic && helm init
          # - kubectl create serviceaccount --namespace kube-system tiller #Create a service account
          # - kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller #Bind the new service account to the cluster-admin role. This will give tiller admin access to the entire cluster
          # - kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}' #Deploy tiller and add the line serviceAccount: tiller to spec.template.spec
          # - helm install --name heapster stable/heapster --namespace kube-system #install Heapster
          - kind delete cluster --name istio-testing #delete the existing cluster

# fleet_script_minikube_docker_tasks : &fleet_script_minikube_docker_tasks #If you are running minikube within a VM, consider using --driver=none
#       script:
#           - |
#             set -eo pipefail #safety for script
#             if [[ $(egrep -c '(vmx|svm)' /proc/cpuinfo) == 0 ]]; then #check if virtualization is supported on Linux, xenial fails w 0, bionic works w 2
#                      echo "virtualization is not supported"
#             else
#                   echo "===================================="
#                   echo eval "$(egrep -c '(vmx|svm)' /proc/cpuinfo)" 2>/dev/null
#                   echo "===================================="
#                   echo "virtualization is supported"
#             fi
#           - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
#           - curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
#           - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
#           - mkdir -p $HOME/.kube $HOME/.minikube
#           - touch $KUBECONFIG
#           - minikube start --profile=minikube --driver=docker --kubernetes-version=v$KUBERNETES_VERSION #the none driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
#           - minikube update-context --profile=minikube
#           - "sudo chown -R travis: /home/travis/.minikube/"
#           - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
#           - echo "=========================================================================================="
#           - kubectl version --client #ensure the version
#           - kubectl cluster-info
#           - minikube status
#           - echo "=========================================================================================="
#           - |
#             echo "Waiting for Kubernetes to be ready ..."
#             for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
#               if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
#                  kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
#                 break
#               fi
#               sleep 2
#             done
#           - echo "============================status check=============================================================="
#           - minikube status
#           - kubectl cluster-info
#           - kubectl get pods --all-namespaces;
#           - kubectl get pods -n default;
#           - echo "=========================================================================================="




# fleet_script_minikube_kvm_latest_tasks : &fleet_script_minikube_kvm_latest_tasks #If you are running minikube within a VM, consider using --driver=none
#       script:
#           - docker version
#           - echo "==========================Check that your CPU supports hardware virtualization================================================================"
#           - egrep -c '(vmx|svm)' /proc/cpuinfo | echo "virtualization is  supported" | echo "virtualization is not supported"
#           - egrep -c ' lm ' /proc/cpuinfo # see if your processor is 64-bit
#           - echo "===============================Installation of KVM===========================================================" #https://help.ubuntu.com/community/KVM/Installation
#           - sudo apt-get install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils
#           - sudo adduser `id -un` libvirt #Bionic (18.04 LTS) and higher
#           - sudo adduser `id -un` kvm
#           - groups #ensure that your username is added to the groups
#           - virsh list --all #Verify Installation
#           - virt-host-validate #Once configured, validate that libvirt reports no errors #https://minikube.sigs.k8s.io/docs/drivers/kvm2/
#           - echo "=========================================================================================="
#           - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
#           - curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
#           - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
#           - mkdir -p $HOME/.kube $HOME/.minikube
#           - touch $KUBECONFIG
#           - sudo minikube start --profile=minikube --driver=kvm2 --kubernetes-version=v$KUBERNETES_VERSION #Start a cluster using the kvm2 driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
#           - minikube config set driver kvm2 #make kvm2 the default driver
#           - minikube update-context --profile=minikube
#           - "sudo chown -R travis: /home/travis/.minikube/"
#           - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
#           - echo "=========================================================================================="
#           - kubectl version --client #ensure the version
#           - kubectl cluster-info
#           - minikube status
#           - echo "=========================================================================================="
#           - |
#             echo "Waiting for Kubernetes to be ready ..."
#             for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
#               if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
#                  kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
#                 break
#               fi
#               sleep 2
#             done
#           - echo "============================status check=============================================================="



fleet_script_minikube_latest_tasks : &fleet_script_minikube_latest_tasks
      script:
          - |
            set -eo pipefail #safety for script
            if [[ $(egrep -c '(vmx|svm)' /proc/cpuinfo) == 0 ]]; then #check if virtualization is supported on Linux, xenial fails w 0, bionic works w 2
                     echo "virtualization is not supported"
            else
                  echo "===================================="
                  echo eval "$(egrep -c '(vmx|svm)' /proc/cpuinfo)" 2>/dev/null
                  echo "===================================="
                  echo "virtualization is supported"
            fi
          - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
          - curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
          - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
          - mkdir -p $HOME/.kube $HOME/.minikube
          - touch $KUBECONFIG
          - sudo minikube start --profile=minikube --vm-driver=none --kubernetes-version=v$KUBERNETES_VERSION #the none driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
          - minikube update-context --profile=minikube
          - "sudo chown -R travis: /home/travis/.minikube/"
          - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
          - echo "=========================================================================================="
          - kubectl version --client #ensure the version
          - kubectl cluster-info
          - minikube status
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
                 kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
                break
              fi
              sleep 2
            done
          - echo "============================status check=============================================================="
          - minikube status
          - kubectl cluster-info
          - kubectl get pods --all-namespaces;
          - kubectl get pods -n default;
          - echo "=========================================================================================="
          - echo "=============================Inspection============================================================="
          - echo "=========================================================================================="
          - kubectl get pod -o wide #The IP column will contain the internal cluster IP address for each pod.
          - kubectl get service --all-namespaces # find a Service IP,list all services in all namespaces
          - docker ps #Find the container ID or name of any container in the pod
          # - docker inspect --format '{{ .State.Pid }}' container-id-or-name #get the process ID of either container, take note of the container ID or name
          # - nsenter -t your-container-pid -n ip addr #advantage of using nsenter to run commands in a pod’s namespace – versus using something like docker exec – is that you have access to all of the commands available on the node
          # - nsenter -t your-container-pid -n ip addr #Finding a Pod’s Virtual Ethernet Interface
          # - curl $CONTAINERIP:8080 #confirm that the web server is still running on port 8080 on the container and accessible from the node
          - echo "=============================Inspecting Conntrack Connection Tracking============================================================="
          # - sudo apt-get -qq -y install conntrack #http://conntrack-tools.netfilter.org/
          - sudo apt-get -qq -y install bridge-utils # Install Linux Bridge Tools.
          - sudo apt-get -qq -y install tcpdump
          - sudo ip address show #List your networking devices
          - sudo ip netns list # list configured network namespaces
          - sudo ip netns add demo-ns #add a namespace called demo-ns
          - sudo ip netns list #see that it's in the list of available namespaces
          #A network namespace is a segregated network environment, complete with its own network stack
          # - echo "=============================start bash in our new namespace demo-ns============================================================="
          # - sudo ip netns exec demo-ns bash #start bash in our new namespace and look for interfaces that it knows about
          # - ping 8.8.8.8 #ping Google's public DNS server
          # #Observe that we have no route out of the namespace, so we don't know how to get to 8.8.8.8 from here
          # - netstat -rn #Check the routes that this namespace knows about
          # # - sudo tcpdump -ni veth0  icmp -c 4 #Confirm that the ping is still running and that both veth0 and cbr0 can see the ICMP packets in the default namespace
          # # - sudo tcpdump -ni eth0  icmp -c 4 #Now check whether eth0 can see the ICMP packets
          # # - sudo sysctl net.ipv4.conf.all.forwarding=1 #Turn forwarding on,Linux, by default, doesn't forward packets between interfaces
          # # - sudo tcpdump -ni eth0  icmp -c 4 #run tcpdump against eth0 to see if fw is working
          # # - sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE #make all outgoing packets from the host look like they're coming from the host's eth0 IP address
          # # - sudo tcpdump -ni eth0  icmp # sniff
          # # - sudo conntrack -L |grep 8.8.8.8 #iptables applies new rules to new flows and leaves ongoing flows alone
          # - ip address show
          # - ip route show
          # - sudo arp #Let's understand how the connectivity looks from the namespace's layer 2 perspective. Confirm that, from demo-ns, the MAC address of192.168.255.1
          # - ping 192.168.255.1 #Attempt to to ping cbr0,From this namespace, we can only see a local loopback interface. We can no longer see or ping eth0 or cbr0.
          # - exit #Exit out of the demo-ns namespace
          # - echo "=============================Exit out of the demo-ns namespace ============================================================="
          - ip address show #Confirm that you can see the interfaces in the default namespace
          - sudo arp #Confirm that you can see the interfaces in the default namespace
          # - sudo tcpdump -ni ens4 icmp -c 4 && sleep 10 #Confirm that you can see the interfaces in the default namespace
          # - sudo conntrack -L | grep 8.8.8.8
          # - conntrack -L #list all the connections currently being tracked
          # - conntrack -E && sleep 5 #watch continuously for new connections
          # - conntrack -L -f ipv4 -d IPADDRESS -o extended #grep conntrack table information using the source IP and Port
          # - kubectl get po — all-namespaces -o wide | grep IPADDRESS #use kubectl to lookup the name of the pod using that Pod IP address
          # - conntrack -D -p tcp --orig-port-dst 80 # delete the relevant conntrack state
          # - sudo conntrack -D -s IPADDRESS
          # - conntrack -L -d IPADDRESS #list conntrack-tracked connections to a particular destination address
          - echo "=============================Inspecting Iptables Rules============================================================="
          - sysctl net.netfilter.nf_conntrack_max #sysctl setting for the maximum number of connections to track
          - sudo sysctl -w net.netfilter.nf_conntrack_max=191000 #set a new valu
          # - sudo iptables-save | head -n 20 #dump all iptables rules on a node
          - sudo iptables -t nat -L KUBE-SERVICES #list just the Kubernetes Service NAT rules
          - echo "=============================Querying Cluster DNS============================================================="
          - sudo apt install dnsutils -y #if dig is not installed
          - kubectl get service -n kube-system kube-dns #find the cluster IP of the kube-dns service CLUSTER-IP
          # - nsenter -t 14346 -n dig kubernetes.default.svc.cluster.local @IPADDRESS #nsenter to run dig in the a container namespace, Service’s full domain name of service-name.namespace.svc.cluster.local
          - sudo apt-get -qq -y install -y ipvsadm
          # - ipvsadm -Ln #list the translation table of IPs ,kube-proxy can configure IPVS to handle the translation of virtual Service IPs to pod IPs
          # - ipvsadm -Ln -t IPADDRESS:PORT #show a single Service IP
          - echo "=========================================================================================="

fleet_script_minikube_tasks : &fleet_script_minikube_tasks
      script:
          # overriding global env variables
          - |
            set -eo pipefail #safety for script
            if [[ $(egrep -c '(vmx|svm)' /proc/cpuinfo) == 0 ]]; then #check if virtualization is supported on Linux, xenial fails w 0, bionic works w 2
                     echo "virtualization is not supported"
            else
                  echo "===================================="
                  echo eval "$(egrep -c '(vmx|svm)' /proc/cpuinfo)" 2>/dev/null
                  echo "===================================="
                  echo "virtualization is supported"
            fi
          - export MINIKUBE_WANTUPDATENOTIFICATION=false
          - export MINIKUBE_WANTREPORTERRORPROMPT=false
          - export CHANGE_MINIKUBE_NONE_USER=true
          - export KUBECTL_VERSION=1.18.3
          - export KUBERNETES_VERSION=1.18.3
          - export MINIKUBE_VERSION=1.18.3
          - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
          - curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v$KUBECTL_VERSION/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
          - curl -Lo minikube https://storage.googleapis.com/minikube/releases/v$MINIKUBE_VERSION/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
          - mkdir -p $HOME/.kube $HOME/.minikube
          - touch $KUBECONFIG
          - sudo minikube start --profile=minikube --vm-driver=none --kubernetes-version=v$KUBERNETES_VERSION #--vm-driver=none, doesn't use a VM, but containers the none driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
          - minikube update-context --profile=minikube
          - "sudo chown -R travis: /home/travis/.minikube/"
          - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
          - echo "=========================================================================================="
          - kubectl version --client #ensure the version
          - kubectl cluster-info
          - minikube status
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
                 kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
                break
              fi
              sleep 2
            done
          - |
            JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}';
            until kubectl -n kube-system get pods -lk8s-app=kube-dns -o jsonpath="$JSONPATH" 2>&1 | grep -q "Ready=True"; do
              sleep 1;
              echo "waiting for kube-dns to be available";
              kubectl get pods --all-namespaces;
            done
          - kubectl run travis-example --image=redis --labels="app=travis-example" # Create example Redis deployment on Kubernetes.
          - | # Make sure created pod is scheduled and running.
            JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}';
            until kubectl -n default get pods -lapp=travis-example -o jsonpath="$JSONPATH" 2>&1 | grep -q "Ready=True"; do
              sleep 1;
              echo "waiting for travis-example deployment to be available";
              kubectl get pods -n default;
            done
          - echo "============================================="
          - kubectl get pods --all-namespaces;
          - kubectl get pods -n default;
          - echo "============================================="
fleet_script_tasks : &fleet_script_tasks
      script:
        - python --version
fleet_install_tasks : &fleet_install_tasks
      install:
        - pip install -r requirements.txt


matrix:
  fast_finish: true
  include:


    # - name: "kind istio  Python 3.7 on bionic" #
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_minikube_gadener_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "minikube gardener istio  Python 3.7 on bionic" #
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_minikube_gadener_tasks
    #   after_success:
    #     - deactivate

    - name: "kind gardener istio  Python 3.7 on bionic" #
      dist: bionic
      language: python
      python: 3.7
      before_install:
        - pip3 install virtualenv
        - virtualenv -p $(which python3) ~venvpy3
        - source ~venvpy3/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      <<: *fleet_script_kind_gardener_tasks
      after_success:
        - deactivate



    # - name: "kind istio  Python 3.7 on bionic" #OK
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_kind_istio_tasks
    #   after_success:
    #     - deactivate



    # - name: "minikube latest  Python 3.7 on bionic" #OK
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_minikube_latest_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "minikube latest  Python 3.7 on xenial" #OK
    #   dist: xenial
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_minikube_latest_tasks
    #   after_success:
    #     - deactivate

    # - name: "Python 3.7 on bionic arm64" # package architecture (amd64) does not match system (arm64)
    #   os: linux
    #   arch: arm64
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on bionic ppc64le" #Unable to locate package osquery
    #   os: linux
    #   arch: ppc64le
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on bionic s390x" #Unable to locate package osquery
    #   os: linux
    #   arch: s390x
    #   dist: bionic
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate


    # - name: "Python 2.7 on xenial amd64"
    #   dist: xenial
    #   language: python
    #   python: 2.7
    #   before_install:
    #     - pip install virtualenv
    #     - virtualenv -p $(which python2) ~venvpy2
    #     - source ~venvpy2/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #
    #   after_success:
    #     - deactivate

    # - name: "Python 3.7 on xenial arm64"
    #   os: linux
    #   arch: arm64
    #   dist: xenial
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on xenial ppc64le" #Unable to locate package osquery
    #   os: linux
    #   arch: ppc64le
    #   dist: xenial
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on xenial s390x" #Unable to locate package osquery
    #   os: linux
    #   arch: s390x
    #   dist: xenial
    #   language: python
    #   python: 3.7
    #   # env:
    #   #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
    #   # compiler:
    #   #  - gcc
    #   #  - clang
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_script_vagrant_tasks
    #   after_success:
    #     - deactivate




    # - name: "Python 2.7.17 on macOS xcode10.2"
    #   os: osx
    #   osx_image: xcode10.2
    #   language: shell
    #   before_install:
    #     - pip install virtualenv
    #     - virtualenv -p $(which python2) ~venvpy2
    #     - source ~venvpy2/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate
    #
    #
    #
    #
    # - name: "Python 3.7.5 on macOS xcode10.2"
    #   os: osx
    #   osx_image: xcode10.2
    #   language: shell
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7.5 on macOS xcode9.4 "
    #   os: osx
    #   osx_image: xcode9.4
    #   language: shell
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate



    # - name: "Python 3.8 on Windows"
    #   os: windows
    #   language: shell
    #   env:
    #     - PATH=/c/Python38:/c/Python38/Scripts:$PATH
    #   before_install:
    #     - choco install python --version 3.8.1
    #     - pip install virtualenv
    #     - virtualenv $HOME/venv
    #     - source $HOME/venv/Scripts/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate
    #
    # - name: "Python 3.7 on Windows"
    #   os: windows
    #   language: shell
    #   env: PATH=/c/Python37:/c/Python37/Scripts:$PATH
    #   before_install:
    #     - choco install python --version 3.7.3
    #     - python -m pip install virtualenv
    #     - virtualenv $HOME/venv
    #     - source $HOME/venv/Scripts/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   after_success:
    #     - deactivate
